<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Warrick Ball</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="Warrick Ball" lang="en" content="Warrick Ball's personal webpage">
    <meta name="author" content="Warrick Ball">
    <meta name="robots" content="index, follow">
    <link rel="stylesheet" type="text/css" href="style.css" />
  </head>
  <body>
    <div class="container">
      <div class="header">
	<h1>Warrick Ball</h1>
	<!-- <h4>Postdoc, University of Birmingham</h4> -->
	<p style="text-align: center">Stellar modeller and asteroseismologist</p>
	<ul id="inline">
	  <li><a href="index.html">Home</a></li>
	  <li><a href="publications.html">Publications</a></li>
	</ul>
      </div>
      <div class="content">
<h1 id="writing-faster-code">Writing faster code</h1>
<p>These notes cover some basics about the two principal aspects of speeding up your code:</p>
<ol type="1">
<li>basic tools for identifying bottlenecks and</li>
<li>some tips for how to actually speed up the code.</li>
</ol>
<p>I’m not an expert at either of these and will only cover basics because that’s all I know myself.</p>
<p>This posts has some snippets entered at the command line, which I indicate with a <code>$</code> as in</p>
<pre><code>$ echo hello
hello</code></pre>
<p>and some code entered into an IPython console, which I indicate with the default prompt <code>In [x]:</code> as in</p>
<pre><code>In [1]: print(&#39;hello&#39;)
hello</code></pre>
<h2 id="context">Context</h2>
<p>Before we start, it’s worth bearing in mind the particular context I’m assuming and what that implies. I’ll discuss …</p>
<h3 id="scientific-code">… scientific code …</h3>
<p>I’m assuming that we’re talking about codes that either analyse or generate data, usually by doing lots of numerical work on data organised and stored in tables, arrays or matrices.</p>
<p>I also assume that we’re optimising for speed, i.e. we’re trying to reduce runtime. Under different circumstances, you might try to optimise e.g. RAM usage, number of I/O operations or network usage. Optimising one metric might or might not help another.</p>
<p>Though most of us do, it’s worth saying that I also assume we’re talking about standard laptop/desktop/cluster hardware, which might also affect what code is faster or slower. In case this point isn’t clear, consider a program that is overflowing RAM and starting to use a swap file on the hard disk. You’ll see different bottlenecks if you have an SSD compared with an optical disk.</p>
<h3 id="in-python-scripts">… in Python scripts …</h3>
<p>I know many people use Jupyter notebooks, for which I’m sure there are profiling tools that I don’t know about. But you can always copy the code you want to optimise into a Python script.</p>
<p>I’ll also say a little bit about compiled code and a few command line tools.</p>
<h3 id="that-already-works-correctly.">… that already works correctly.</h3>
<p>There is no point optimising code that doesn’t already produce the correct result. Put differently, it’s trivially easy to write <a href="https://xkcd.com/221/">very fast code that produces garbage</a>. e.g.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="kw">def</span> randint():</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a>    <span class="cf">return</span> <span class="dv">4</span></span></code></pre></div>
<p>More importantly, you don’t want to break your code by speeding it up. The best way to avoid this is to have unit tests that cover most (ideally all) of the code and run regularly (ideally automatically).</p>
<h2 id="profiling">Profiling</h2>
<p>Before you start trying to make your code faster, you need to know what part is slow. Even if you speed things up by a factor of 100 in a part of the code that contributes 1% to the runtime, you’ll still only reduce the overall runtime by 0.99%.</p>
<p>But that does <em>not</em> mean you shouldn’t try to find what is dominating the runtime, speed that up and, having done so, profile again to find the next bottleneck.</p>
<p>Also, remember to try profiling your code on varying amounts of data. You might find different bottlenecks on an array of 10⁴ elements as an array of 10⁸ elements. Generally, try to profile as realistic run as you can afford time.</p>
<h3 id="simple-tools">Simple tools</h3>
<p>There are a few simple tools that tell you how long a given snippet of Python takes to run. IPython provides the magic functions <code>%time</code> and <code>%timeit</code>, which use the <a href="https://docs.python.org/3/library/timeit.html"><code>timeit</code></a> module from the standard library. <code>%time</code> on its own just tells you how long a single line of code takes to run. e.g.</p>
<pre><code>In [1]: %time x = [i**2.2 for i in range(10_000_000) if i%3==1]
CPU times: user 920 ms, sys: 42.6 ms, total: 962 ms
Wall time: 957 ms</code></pre>
<p><code>%timeit</code> will instead run the snippet of code a number of times to give a mean and standard deviation. e.g.</p>
<pre><code>In [2]: %timeit x = [i**2.2 for i in range(10_000_000) if i%3==1]
931 ms ± 3.33 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre>
<p><code>%timeit</code> decides how many times to run the code based on how long it takes.</p>
<p>At the command line, we can use <code>time &lt;command&gt;</code> to see how long <code>&lt;command&gt;</code> takes. e.g.</p>
<pre><code>$ time sleep 1

real    0m1.003s
user    0m0.000s
sys     0m0.003s</code></pre>
<p>which is what we expect because <code>sleep &lt;n&gt;</code> simply does nothing for <code>n</code> seconds.</p>
<h3 id="cprofile">cProfile</h3>
<p><a href="https://docs.python.org/3/library/profile.html">cProfile</a> is a simple and standard profiler whose main advantage is that it’s part of Python’s standard library. If you have Python, you have cProfile.</p>
<p>Consider this very simple script called <code>script.py</code>:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a><span class="co">#!/usr/bin/env python3</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true"></a>A <span class="op">=</span> np.random.rand(<span class="dv">2000</span>,<span class="dv">1000</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true"></a>b <span class="op">=</span> np.random.rand(<span class="dv">2000</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true"></a>Ainv <span class="op">=</span> np.linalg.pinv(A)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true"></a>x <span class="op">=</span> Ainv.dot(b)</span></code></pre></div>
<p>We run cProfile on it with</p>
<pre><code>$ python3 -m cProfile script.py
         76569 function calls (74416 primitive calls) in 0.837 seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    0.000    0.000 &lt;__array_function__ internals&gt;:2(&lt;module&gt;)
        1    0.000    0.000    0.000    0.000 &lt;__array_function__ internals&gt;:2(amax)
        1    0.000    0.000    0.000    0.000 &lt;__array_function__ internals&gt;:2(concatenate)
        1    0.000    0.000    0.000    0.000 &lt;__array_function__ internals&gt;:2(copyto)
        1    0.000    0.000    0.711    0.711 &lt;__array_function__ internals&gt;:2(pinv)
        1    0.000    0.000    0.670    0.670 &lt;__array_function__ internals&gt;:2(svd)
        2    0.000    0.000    0.000    0.000 &lt;__array_function__ internals&gt;:2(swapaxes)
    150/1    0.001    0.000    0.109    0.109 &lt;frozen importlib._bootstrap&gt;:1002(_find_and_load)
   178/15    0.000    0.000    0.106    0.007 &lt;frozen importlib._bootstrap&gt;:1033(_handle_fromlist)
      448    0.001    0.000    0.001    0.000 &lt;frozen importlib._bootstrap&gt;:112(release)
      150    0.000    0.000    0.000    0.000 &lt;frozen importlib._bootstrap&gt;:152(__init__)
      150    0.000    0.000    0.001    0.000 &lt;frozen importlib._bootstrap&gt;:156(__enter__)
      150    0.000    0.000    0.001    0.000 &lt;frozen importlib._bootstrap&gt;:160(__exit__)
      448    0.001    0.000    0.001    0.000 &lt;frozen importlib._bootstrap&gt;:166(_get_module_lock)
      227    0.000    0.000    0.000    0.000 &lt;frozen importlib._bootstrap&gt;:185(cb)
...</code></pre>
<p>where I’ve only showed the first 20 lines of output. So, what does this say? Each row gives the following information about calls to a specific function:</p>
<ul>
<li><code>ncalls</code>: the number of calls to that function,</li>
<li><code>tottime</code>: the number of seconds spent in that function but <em>not</em> the functions it called,</li>
<li><code>cumtime</code>: the number of seconds spent in that function <em>including</em> the functions it called,</li>
<li><code>percall</code>: <code>tottime</code> or <code>cumtime</code> divided by <code>ncalls</code>, and</li>
<li><code>filename:lineno(function)</code>: the name of the function and which line it occurs on in the given source file.</li>
</ul>
<p>By default, the function calls are sorted alphabetically by name, which usually isn’t what we want. To sort by a different key, you can pass the <code>-s &lt;column name&gt;</code> flag to cProfile. e.g.</p>
<pre><code>$ python3 -m cProfile -s tottime script.py
         76569 function calls (74416 primitive calls) in 0.903 seconds

   Ordered by: internal time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.738    0.738    0.738    0.738 linalg.py:1482(svd)
        1    0.038    0.038    0.776    0.776 linalg.py:1915(pinv)
      320    0.020    0.000    0.020    0.000 {built-in method builtins.compile}
        2    0.016    0.008    0.016    0.008 {method &#39;rand&#39; of &#39;numpy.random.mtrand.RandomState&#39; objects}
      107    0.011    0.000    0.011    0.000 {built-in method marshal.loads}
    31/29    0.009    0.000    0.011    0.000 {built-in method _imp.create_dynamic}
  230/229    0.004    0.000    0.006    0.000 {built-in method builtins.__build_class__}
      687    0.003    0.000    0.003    0.000 {built-in method posix.stat}
      382    0.003    0.000    0.010    0.000 &lt;frozen importlib._bootstrap_external&gt;:1438(find_spec)
      107    0.002    0.000    0.002    0.000 {built-in method io.open_code}
      107    0.002    0.000    0.002    0.000 {method &#39;read&#39; of &#39;_io.BufferedReader&#39; objects}
      620    0.001    0.000    0.002    0.000 _inspect.py:65(getargs)
    31/21    0.001    0.000    0.008    0.000 {built-in method _imp.exec_dynamic}
     1905    0.001    0.000    0.003    0.000 &lt;frozen importlib._bootstrap_external&gt;:62(_path_join)
     3956    0.001    0.000    0.001    0.000 {built-in method builtins.getattr}
...</code></pre>
<p>where we can now see that most of the time is spent computing the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition (SVD)</a>, presumably to compute the <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">pseudo-inverse</a> (<code>pinv</code>), followed by generating random numbers. In a real program, you might then decide to have a look at whether that matrix needs inverting or if you could do something else that’s faster.</p>
<p>You can also visualise the output with a standard visualiser called <a href="http://kcachegrind.sourceforge.net/html/Home.html">KCachegrind</a>. I’ll assume you’ve somehow installed it. In Linux, it should be available from the system’s repos (e.g. <code>dnf install kcachegrind</code> or <code>apt install kcachegrind</code>). If you don’t want to pull all the KDE dependencies, you can try the equivalent <code>qcachegrind</code>.</p>
<p>First, we need to create the binary profile data with cProfile using the <code>-o &lt;binary file name&gt;</code> flag. e.g.</p>
<pre><code>$ python3 -m cProfile -o script.prof script.py</code></pre>
<p>We then call <a href="https://pypi.org/project/pyprof2calltree/"><code>pyprof2calltree</code></a> on the output, where the <code>-i</code> flag indicates the input file and the <code>-k</code> flag tells it to run KCachegrind.</p>
<pre><code>$ pyprof2calltree -i script.prof -k</code></pre>
<p><code>pyprof2calltree</code> is not a standard package but you can download it wherever you get your Python packages.</p>
<h3 id="valgrind">Valgrind</h3>
<p>For compiled code in many languages (notably including C, C++ or Fortran), there’s a standard profiler called <a href="https://valgrind.org/">Valgrind</a>. It’s actually a suite of tools and the one for profiling runtime is <code>callgrind</code>. You should be able to install Valgrind from your system packages. You can then run it on a compiled program called <code>./program</code> with</p>
<pre><code>$ valgrind --tool=callgrind ./program</code></pre>
<p>Your program will then run <em>very very slowly</em> and produce a file called something like <code>callgrind.out.&lt;pid&gt;</code>. The output is produced as the program runs, so if it’s taking too long, you can interrupt it with <code>Ctrl-c</code> and still look at whatever was produced up to that point.</p>
<p>The output file is in an unintelligible binary format but can also be visualised with KCachegrind by running e.g.</p>
<pre><code>$ kcachegrind callgrind.out.&lt;pid&gt;</code></pre>
<p>(or <code>qcachegrind</code> if you installed that instead of <code>kcachegrind</code>).</p>
<h2 id="broad-strategies">Broad strategies</h2>
<p>This is a somewhat random collection of things you can try to speed up a block of code. They are roughly in order of increasing difficulty but it depends on your skills and how the code works.</p>
<h3 id="avoid-loops">Avoid loops</h3>
<p>Python for-loops are notoriously slow, though I think this is probably true of many interpreted languages. (It was true of MATLAB the last time I used it.) Because we work with arrays, you might be able to replace a loop over two arrays with operations directly on the arrays themselves (we say we <em>vectorise</em> the operation), perhaps by using some logical indexing or broadcasting. The gains are potentially huge: eliminating a for loop often speeds things up by an order of magnitude or more.</p>
<p>Though a bit contrived, this is what we’re talking about:</p>
<pre><code>In [1]: import numpy as np

In [2]: x = np.random.rand(1_000_000)

In [3]: %timeit y = np.array([np.sin(xi**2) for xi in x])
1.57 s ± 16.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [4]: %timeit y = np.sin(x**2)
13.4 ms ± 174 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)</code></pre>
<h3 id="dont-repeat-expensive-things">Don’t repeat expensive things</h3>
<p>This might seem obvious but it requires knowing what part of the code is expensive and whether you can afford to store an intermediate result (which might require a lot of memory or something).</p>
<p>Note that special functions can be quite expensive. Even trigonometric functions are much slower than the basic operations <code>+</code>, <code>-</code> and <code>*</code>, so e.g. rather use <code>np.sin(2*x)</code> than <code>2*np.sin(x)*np.cos(x)</code>.</p>
<h3 id="try-magic-packages">Try “magic” packages</h3>
<p>There are several Python packages that will potentially speed up your code dramatically with relatively small changes that don’t damage readability. These are usually worth a try because they’re so simple but I seldom get the advertised performance gains when working with arrays. A few example packages are</p>
<ul>
<li><a href="https://numba.readthedocs.io/">Numba</a></li>
<li><a href="https://numexpr.readthedocs.io/">NumExpr</a></li>
<li><a href="https://bottleneck.readthedocs.io/">Bottleneck</a></li>
</ul>
<p>but there are probably more that I don’t know about.</p>
<h3 id="parallelise">Parallelise</h3>
<p>Most modern computers have multiple cores, so one way to speed things up is to make use of them. But only do this if the libraries you are using for the slow parts of your code aren’t already parallelised!</p>
<p>The standard module for parallelising in Python is <a href="https://docs.python.org/3/library/multiprocessing.html"><code>multiprocessing</code></a> but the overhead can be large enough that you don’t see a gain compared to array operations. In Fortran or C++, it’s sometimes quite easy to parallelise with <a href="https://www.openmp.org/">OpenMP</a>. If you need more advanced parallelisation, your skills probably far exceed the content of this article.</p>
<p>If you can parallelise, you should try to do so at the highest level that you can, so I tend to parallelise the execution of my scripts using the command line tool <code>xargs</code>. The main purpose of <code>xargs</code> is to take the output from one command line program and provide it as input arguments to the next. e.g. consider <code>seq</code>, which prints a list of numbers.</p>
<pre><code>$ seq 4
1
2
3
4</code></pre>
<p>Let’s say we want to provide each of these numbers as an argument to, say, <code>echo</code>.</p>
<pre><code>$ seq 4 | xargs echo
1 2 3 4</code></pre>
<p>Wait, what? <code>xargs</code> defaults to sending all the output from the previous program to one instance of the next one. We can confirm this by using <code>xargs</code>’s <code>-t</code> flag, which will show us what <code>xargs</code> has decided to execute (I usually use <code>-t</code>):</p>
<pre><code>$ seq 4 | xargs -t echo
echo 1 2 3 4 
1 2 3 4</code></pre>
<p>If we want to send only one argument to each instance of <code>xargs</code>, we can use the <code>-n &lt;N&gt;</code> flag, where <code>&lt;N&gt;</code> is the number of arguments we want to send to each instance of <code>echo</code>.</p>
<pre><code>$ seq 4 | xargs -t -n 1 echo
echo 1 
1
echo 2 
2
echo 3 
3
echo 4 
4</code></pre>
<p>Let’s get back to parallelisation. First, try passing the numbers to <code>sleep</code> and use <code>time</code> to see how long it takes.</p>
<pre><code>$ time seq 4 | xargs -t -n 1 sleep
sleep 1 
sleep 2 
sleep 3 
sleep 4 

real    0m10.006s
user    0m0.003s
sys     0m0.004s</code></pre>
<p>That makes sense, because we ran sleep for 1s+2s+3s+4s=10s. To use <code>xargs</code> to run <code>sleep</code> in parallel, we use the <code>-P &lt;N&gt;</code> flag, where now <code>&lt;N&gt;</code> is the number of jobs we’d like to use.</p>
<pre><code>$ time seq 4 | xargs -t -n 1 -P 4 sleep
sleep 1 
sleep 2 
sleep 3 
sleep 4 

real    0m4.003s
user    0m0.002s
sys     0m0.004s</code></pre>
<p>Again, this makes sense because my computer runs all 4 <code>sleep</code>s in parallel, so we just have to wait for the last one to finish, which takes 4s.</p>
<h3 id="link-compiled-code">Link compiled code</h3>
<p>The final change you can try making to your code is to call compiled code directly from Python. The standard tool to call Fortran is <a href="https://numpy.org/doc/stable/f2py/">F2PY</a>, which is part of NumPy.</p>
<p>There are many options for C and C++. I don’t really know C or C++ so haven’t used them but as far as I know some options are <a href="https://docs.python.org/3/library/ctypes.html">ctypes</a>, <a href="https://cython.readthedocs.io/">Cython</a>, <a href="https://cffi.readthedocs.io/">CFFI</a> and <a href="https://pybind11.readthedocs.io/">PyBind</a>. Also have a look at <a href="https://dfm.io/posts/python-c-extensions/">this post</a> by Daniel Foreman-Mackay.</p>
<h3 id="rethink">Rethink</h3>
<p>If no tricks are working, it might be worth getting away from the code and rethinking the overall algorithm you’re using and whether there are things you can change. Perhaps you can do an MCMC burn-in with an approximate calculation? Or perhaps you can replace a section with a difficult but fast analytic result?</p>
<h2 id="some-specific-things">Some specific things</h2>
<p>It’s impossible to make a list of every change that might speed up your code but here are some things that I often run into or work around.</p>
<h3 id="python-imports-can-take-the-better-part-of-a-second.">Python imports can take the better part of a second.</h3>
<p>Consider this simple script that just import a few things we might want to use in our analysis.</p>
<pre><code>$ cat imports.py 
#!/usr/bin/env python3

import numpy as np
from scipy import optimize
from scipy import integrate
from astropy.io import fits
from astropy.timeseries import LombScargle</code></pre>
<p>How long does it take to run?</p>
<pre><code>$ time python3 ../faster_code/imports.py 

real    0m0.564s
user    0m0.776s
sys     0m0.295s</code></pre>
<p>A bit over half a second. That might not seem like much but if you’re running a script to do half a second of work (besides the imports) on thousands of files, you might spend hours just importing Python modules.</p>
<p>My standard solution is to write scripts that take lists of filenames as arguments and run the scripts over multiple files (perhaps dozens) on each call, usually using <code>xargs</code>.</p>
<h3 id="binary-io-is-usually-faster-than-plain-text-io.">Binary I/O is usually faster than plain-text I/O.</h3>
<p>It’s usually faster to read and write data in binary formats (e.g. NumPy binaries, FITS, HDF5) than plain text (e.g. CSV). So if you’re repeatedly working with a plain text file that’s a bit slow to read, take a moment to convert it into a binary format.</p>
<p>Here’s a simple demonstration:</p>
<pre><code>In [1]: import numpy as np

In [2]: x = np.random.rand(1_000_000)

In [3]: np.savetxt(&#39;x.txt&#39;, x)

In [4]: %timeit y = np.loadtxt(&#39;x.txt&#39;)
2.84 s ± 27.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

In [5]: np.save(&#39;x.npy&#39;, x)

In [6]: %timeit y = np.load(&#39;x.npy&#39;)
1.75 ms ± 49 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)</code></pre>
<p>Though somewhat contrived, you really can speed things up by an order of magnitude or more.</p>
<p>Binary files are also usually somewhat smaller and occasionally <em>much</em> smaller than the corresponding plain text files.</p>
<h3 id="network-io-can-be-slow-for-lots-of-small-files.">Network I/O can be slow for lots of small files.</h3>
<p>Since we’re all working from home, you’re possibly spending some big data files over the Internet. This may or may not be a problem. If you use a decent broadband internet connection to fetch one file, you might not notice anything. But I find things get quite slow when accessing multiple smaller files, in which case you can either somehow bundle your files up on the server so you can read them once, or just take the plunge and copy them to your hard drive (if you can afford the space).</p>
<h3 id="know-when-to-stop">Know when to stop</h3>
<p>Computers run at a finite speed; there’s a limit to how fast any particular piece of code will run. If your code already spends most of its time in the part that <em>should</em> take the most time, it’s worth considering whether there’s anything more to be done.</p>
      </div>
    </div>
  </body>
</html>
